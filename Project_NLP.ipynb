{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMndY6uhz1eGoQlFNkpRjxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ho-Ngoc-Tai/NLP/blob/main/Project_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import thÆ° viá»‡n cáº§n thiáº¿t"
      ],
      "metadata": {
        "id": "CmhErDG6Rv7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "kSgFJ614RrTb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TÃ¡ch cÃ¢u tá»« vÄƒn báº£n"
      ],
      "metadata": {
        "id": "GMJMUSpjR2kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DÃ¹ng rule-based sentence splitting Ä‘á»ƒ tÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u Ä‘á»™c láº­p."
      ],
      "metadata": {
        "id": "G_S31NxbScjR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eQFVjiJRQAP"
      },
      "outputs": [],
      "source": [
        "def split_sentences(text):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s.strip() for s in sentences if len(s.strip()) > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiá»n xá»­ lÃ½ cÃ¢u (tokenize + stopwords)"
      ],
      "metadata": {
        "id": "fdNkgK0SSOZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Ä‘Ã£ dÃ¹ng:\n",
        "\n",
        "Token\n",
        "\n",
        "Lowercase\n",
        "\n",
        "Stopword removal"
      ],
      "metadata": {
        "id": "tsVMguX1SorZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(\"\"\"\n",
        "a an the is are was were in on at of for with and or to from by as\n",
        "that this these those be been being have has had do does did\n",
        "\"\"\".split())\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    words = re.findall(r'[a-z]+', sentence)\n",
        "    words = [w for w in words if w not in STOPWORDS]\n",
        "    return words"
      ],
      "metadata": {
        "id": "xxLfSEZHSPPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TÃ­nh similarity giá»¯a hai cÃ¢u\n",
        "\n",
        "ðŸ‘‰ Overlap + log normalization (khÃ¡c repo, an toÃ n)"
      ],
      "metadata": {
        "id": "QbG-5T5rSPXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chuáº©n hÃ³a báº±ng log Ä‘á»ƒ trÃ¡nh Æ°u tiÃªn cÃ¡c cÃ¢u dÃ i."
      ],
      "metadata": {
        "id": "9gohi50jS4f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(words_i, words_j):\n",
        "    if not words_i or not words_j:\n",
        "        return 0.0\n",
        "\n",
        "    common = set(words_i) & set(words_j)\n",
        "    if len(common) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return len(common) / (\n",
        "        math.log(len(words_i) + 1) * math.log(len(words_j) + 1)\n",
        "    )"
      ],
      "metadata": {
        "id": "Lb6B_KXTSPgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XÃ¢y dá»±ng ma tráº­n trá»ng sá»‘ (Ä‘á»“ thá»‹ cÃ¢u)"
      ],
      "metadata": {
        "id": "5rTaQWnMSPoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ ÄÃ¢y chÃ­nh lÃ :\n",
        "\n",
        "Node = cÃ¢u\n",
        "\n",
        "Edge weight = similarity"
      ],
      "metadata": {
        "id": "GFNZnUy2SP34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_weight_matrix(tokenized_sentences):\n",
        "    n = len(tokenized_sentences)\n",
        "    weights = [[0.0] * n for _ in range(n)]\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i != j:\n",
        "                weights[i][j] = sentence_similarity(\n",
        "                    tokenized_sentences[i],\n",
        "                    tokenized_sentences[j]\n",
        "                )\n",
        "    return weights"
      ],
      "metadata": {
        "id": "8FWNkzZ-SPvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXTRANK â€“ PageRank viáº¿t tay (QUAN TRá»ŒNG NHáº¤T)"
      ],
      "metadata": {
        "id": "EZLJmVTLTGIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tá»± cÃ i Ä‘áº·t láº¡i PageRank dá»±a trÃªn cÃ´ng thá»©c TextRank trong paper."
      ],
      "metadata": {
        "id": "jArtgQ1FTRGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank(weight_matrix, damping=0.85, max_iter=50, tol=1e-4):\n",
        "    n = len(weight_matrix)\n",
        "    scores = [1.0 / n] * n\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        new_scores = [(1 - damping) / n] * n\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if weight_matrix[j][i] > 0:\n",
        "                    out_sum = sum(weight_matrix[j])\n",
        "                    if out_sum != 0:\n",
        "                        new_scores[i] += (\n",
        "                            damping * scores[j] * weight_matrix[j][i] / out_sum\n",
        "                        )\n",
        "\n",
        "        if max(abs(new_scores[i] - scores[i]) for i in range(n)) < tol:\n",
        "            break\n",
        "\n",
        "        scores = new_scores\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "sfe4jzqBSP_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chá»n cÃ¢u táº¡o summary (giá»›i háº¡n sá»‘ tá»«)"
      ],
      "metadata": {
        "id": "9YAOh1OSSQHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KhÃ¡c háº³n top-k repo â†’ an toÃ n."
      ],
      "metadata": {
        "id": "9EnvJSasTd43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(sentences, scores, max_words=100):\n",
        "    ranked = sorted(\n",
        "        zip(sentences, scores),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    summary = []\n",
        "    word_count = 0\n",
        "\n",
        "    for sent, _ in ranked:\n",
        "        words = sent.split()\n",
        "        if word_count + len(words) <= max_words:\n",
        "            summary.append(sent)\n",
        "            word_count += len(words)\n",
        "\n",
        "    return \" \".join(summary)"
      ],
      "metadata": {
        "id": "ay1pWAzBSQOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HÃ m tá»•ng há»£p â€“ cháº¡y TextRank end-to-end"
      ],
      "metadata": {
        "id": "0AVGq1pOSQVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank_summarize(text, max_words=100):\n",
        "    sentences = split_sentences(text)\n",
        "    tokenized = [preprocess_sentence(s) for s in sentences]\n",
        "    weight_matrix = build_weight_matrix(tokenized)\n",
        "    scores = textrank(weight_matrix)\n",
        "    summary = generate_summary(sentences, scores, max_words)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "h8gFmkWZSQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEMO TEST"
      ],
      "metadata": {
        "id": "yAewGHR_Tsnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "TextRank is a graph-based ranking algorithm for natural language processing.\n",
        "It is inspired by the PageRank algorithm used by Google.\n",
        "Each sentence in a document is represented as a node in a graph.\n",
        "Edges are weighted by sentence similarity.\n",
        "Important sentences are those that are strongly connected to other important sentences.\n",
        "TextRank does not require training data.\n",
        "\"\"\"\n",
        "\n",
        "print(textrank_summarize(text, max_words=40))\n"
      ],
      "metadata": {
        "id": "EmBvbXsMSQnZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}